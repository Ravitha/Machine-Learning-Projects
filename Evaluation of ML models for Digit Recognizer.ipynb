{"cells":[{"metadata":{"_uuid":"971dad4d-c433-4bc9-ae59-20e6bac3a473","_cell_guid":"fd0fcc8e-5a85-4267-bc1f-02e1a8744c6d","trusted":true},"cell_type":"code","source":"# Classifying Digits using Simple ML models\n# Necessary Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\n\n# Exploring files in the Input Directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Step 1: Convert the data into usable format\n# pandas is used to read the contents of csv into a dataframe\ntrain = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n\n# Retrieve the features and label columns into a separate numpy arrays\nfeatures = train[train.columns[1:]].values\nlabel = train.label.values\n\nprint(type(features))\nprint(type(label))\n\nprint(features.shape)\nprint(label.shape)\n","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/digit-recognizer/test.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/sample_submission.csv\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n(42000, 784)\n(42000,)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Model 1: **\n\nLogistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\")\nn_splits = 5\nkf = KFold(n_splits=n_splits, random_state=137)\nacc = 0.0\nglobal best_model\n\nfor jj, (train_index, val_index) in enumerate(kf.split(features)):\n    print(\"Fitting fold\", jj+1)\n    train_features = features[train_index]\n    train_target = label[train_index]\n    \n    val_features = features[val_index]\n    val_target = label[val_index]\n    \n    model = LogisticRegression(C=20, solver='lbfgs', multi_class='multinomial')\n    model.fit(train_features, train_target)\n    val_pred = model.predict_proba(val_features)\n    fold_acc=accuracy_score(val_target, np.argmax(val_pred, axis=1))\n    print(\"Fold accuracy:\", accuracy_score(val_target, np.argmax(val_pred, axis=1)))\n    #test_preds += model.predict_proba(test)/n_splits\n    if(fold_acc>acc):\n        acc = fold_acc\n        best_model = model\n    del train_features, train_target, val_features, val_target\n    gc.collect()\n\n    \n    ","execution_count":2,"outputs":[{"output_type":"stream","text":"Fitting fold 1\nFold accuracy: 0.9176190476190477\nFitting fold 2\nFold accuracy: 0.9191666666666667\nFitting fold 3\nFold accuracy: 0.911547619047619\nFitting fold 4\nFold accuracy: 0.9133333333333333\nFitting fold 5\nFold accuracy: 0.9157142857142857\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(acc)","execution_count":3,"outputs":[{"output_type":"stream","text":"0.9191666666666667\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\nprint(test.columns)\n#Retrieve the features and label columns into a separate numpy arrays\ntest_features = test[test.columns[0:]].values\ntest_pred = model.predict_proba(test_features)\npredict = np.argmax(test_pred, axis=1)","execution_count":4,"outputs":[{"output_type":"stream","text":"Index(['pixel0', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6',\n       'pixel7', 'pixel8', 'pixel9',\n       ...\n       'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779',\n       'pixel780', 'pixel781', 'pixel782', 'pixel783'],\n      dtype='object', length=784)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/digit-recognizer/sample_submission.csv')\nsubmission['Label'] = predict\nsubmission.to_csv('submission.csv', index=False)","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model 2: **\n\nLogistic Regression with STOCHASTIC GRADIENT DESCENT "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nn_splits = 5\nkf = KFold(n_splits=n_splits, random_state=137)\nacc = 0.0\nglobal best_model\n\nfor jj, (train_index, val_index) in enumerate(kf.split(features)):\n    print(\"Fitting fold\", jj+1)\n    train_features = features[train_index]\n    train_target = label[train_index]\n    \n    val_features = features[val_index]\n    val_target = label[val_index]\n    \n    # loss = 'hinge' represents linear regression\n    # log loss implement logistic regression\n    model = SGDClassifier(loss='log')\n    model.fit(train_features, train_target)\n    val_pred = model.predict_proba(val_features)\n    fold_acc=accuracy_score(val_target, np.argmax(val_pred, axis=1))\n    print(\"Fold accuracy:\", accuracy_score(val_target, np.argmax(val_pred, axis=1)))\n    #test_preds += model.predict_proba(test)/n_splits\n    if(fold_acc>acc):\n        acc = fold_acc\n        best_model = model\n    del train_features, train_target, val_features, val_target\n    gc.collect()\n","execution_count":null,"outputs":[{"output_type":"stream","text":"Fitting fold 1\nFold accuracy: 0.809047619047619\nFitting fold 2\nFold accuracy: 0.7825\nFitting fold 3\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Model 3:**\nRandomForest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nn_splits = 5\nkf = KFold(n_splits=n_splits, random_state=137)\nacc = 0.0\nglobal best_model\n\nfor jj, (train_index, val_index) in enumerate(kf.split(features)):\n    print(\"Fitting fold\", jj+1)\n    train_features = features[train_index]\n    train_target = label[train_index]\n    \n    val_features = features[val_index]\n    val_target = label[val_index]\n    \n    # max depth serves as an important hyperparameter\n    # When the depth was set with a value of 2, accuracy was down by 60%\n    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n    model.fit(train_features, train_target)\n    val_pred = model.predict_proba(val_features)\n    fold_acc=accuracy_score(val_target, np.argmax(val_pred, axis=1))\n    print(\"Fold accuracy:\", accuracy_score(val_target, np.argmax(val_pred, axis=1)))\n    #test_preds += model.predict_proba(test)/n_splits\n    if(fold_acc>acc):\n        acc = fold_acc\n        best_model = model\n    del train_features, train_target, val_features, val_target\n    gc.collect()\n","execution_count":8,"outputs":[{"output_type":"stream","text":"Fitting fold 1\nFold accuracy: 0.9432142857142857\nFitting fold 2\nFold accuracy: 0.9407142857142857\nFitting fold 3\nFold accuracy: 0.9402380952380952\nFitting fold 4\nFold accuracy: 0.9470238095238095\nFitting fold 5\nFold accuracy: 0.9484523809523809\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\nprint(test.columns)\n#Retrieve the features and label columns into a separate numpy arrays\ntest_features = test[test.columns[0:]].values\ntest_pred = model.predict_proba(test_features)\npredict = np.argmax(test_pred, axis=1)","execution_count":9,"outputs":[{"output_type":"stream","text":"Index(['pixel0', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6',\n       'pixel7', 'pixel8', 'pixel9',\n       ...\n       'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779',\n       'pixel780', 'pixel781', 'pixel782', 'pixel783'],\n      dtype='object', length=784)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/digit-recognizer/sample_submission.csv')\nsubmission['Label'] = predict\nsubmission.to_csv('submission.csv', index=False)","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reference:**\nhttps://www.kaggle.com/tunguz/mnist-logistic-regression-baseline"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}