{"cells":[{"metadata":{"_uuid":"971dad4d-c433-4bc9-ae59-20e6bac3a473","_cell_guid":"fd0fcc8e-5a85-4267-bc1f-02e1a8744c6d","trusted":true},"cell_type":"code","source":"# Classifying Digits using Simple ML models\n# Necessary Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\n\n# Exploring files in the Input Directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Step 1: Convert the data into usable format\n# pandas is used to read the contents of csv into a dataframe\ntrain = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n\n# Retrieve the features and label columns into a separate numpy arrays\nfeatures = train[train.columns[1:]].values\nlabel = train.label.values\n\nprint(type(features))\nprint(type(label))\n\nprint(features.shape)\nprint(label.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model 1: **\n\nLogistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\")\nn_splits = 5\nkf = KFold(n_splits=n_splits, random_state=137)\nacc = 0.0\nglobal best_model\n\nfor jj, (train_index, val_index) in enumerate(kf.split(features)):\n    print(\"Fitting fold\", jj+1)\n    train_features = features[train_index]\n    train_target = label[train_index]\n    \n    val_features = features[val_index]\n    val_target = label[val_index]\n    \n    model = LogisticRegression(C=20, solver='lbfgs', multi_class='multinomial')\n    model.fit(train_features, train_target)\n    val_pred = model.predict_proba(val_features)\n    fold_acc=accuracy_score(val_target, np.argmax(val_pred, axis=1))\n    print(\"Fold accuracy:\", accuracy_score(val_target, np.argmax(val_pred, axis=1)))\n    #test_preds += model.predict_proba(test)/n_splits\n    if(fold_acc>acc):\n        acc = fold_acc\n        best_model = model\n    del train_features, train_target, val_features, val_target\n    gc.collect()\n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\nprint(test.columns)\n#Retrieve the features and label columns into a separate numpy arrays\ntest_features = test[test.columns[0:]].values\ntest_pred = model.predict_proba(test_features)\npredict = np.argmax(test_pred, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/digit-recognizer/sample_submission.csv')\nsubmission['Label'] = predict\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model 2: **\n\nLogistic Regression with STOCHASTIC GRADIENT DESCENT "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nn_splits = 5\nkf = KFold(n_splits=n_splits, random_state=137)\nacc = 0.0\nglobal best_model\n\nfor jj, (train_index, val_index) in enumerate(kf.split(features)):\n    print(\"Fitting fold\", jj+1)\n    train_features = features[train_index]\n    train_target = label[train_index]\n    \n    val_features = features[val_index]\n    val_target = label[val_index]\n    \n    # loss = 'hinge' represents linear regression\n    # log loss implement logistic regression\n    model = SGDClassifier(loss='log')\n    model.fit(train_features, train_target)\n    val_pred = model.predict_proba(val_features)\n    fold_acc=accuracy_score(val_target, np.argmax(val_pred, axis=1))\n    print(\"Fold accuracy:\", accuracy_score(val_target, np.argmax(val_pred, axis=1)))\n    #test_preds += model.predict_proba(test)/n_splits\n    if(fold_acc>acc):\n        acc = fold_acc\n        best_model = model\n    del train_features, train_target, val_features, val_target\n    gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model 3:**\nRandomForest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nn_splits = 5\nkf = KFold(n_splits=n_splits, random_state=137)\nacc = 0.0\nglobal best_model\n\nfor jj, (train_index, val_index) in enumerate(kf.split(features)):\n    print(\"Fitting fold\", jj+1)\n    train_features = features[train_index]\n    train_target = label[train_index]\n    \n    val_features = features[val_index]\n    val_target = label[val_index]\n    \n    # max depth serves as an important hyperparameter\n    # When the depth was set with a value of 2, accuracy was down by 60%\n    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n    model.fit(train_features, train_target)\n    val_pred = model.predict_proba(val_features)\n    fold_acc=accuracy_score(val_target, np.argmax(val_pred, axis=1))\n    print(\"Fold accuracy:\", accuracy_score(val_target, np.argmax(val_pred, axis=1)))\n    #test_preds += model.predict_proba(test)/n_splits\n    if(fold_acc>acc):\n        acc = fold_acc\n        best_model = model\n    del train_features, train_target, val_features, val_target\n    gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\nprint(test.columns)\n#Retrieve the features and label columns into a separate numpy arrays\ntest_features = test[test.columns[0:]].values\ntest_pred = model.predict_proba(test_features)\npredict = np.argmax(test_pred, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/digit-recognizer/sample_submission.csv')\nsubmission['Label'] = predict\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model 3:**\nConvolutional Neural Network"},{"metadata":{},"cell_type":"markdown","source":"**Defining a simple convolutional neural network**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch import nn\nclass Flatten(nn.Module):\n    def __init__(self):\n        super(Flatten, self).__init__()\n\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass NN(torch.nn.Module):\n    def __init__(self):\n        super(NN,self).__init__()\n        self.conv1 = torch.nn.Conv2d(1,6,3,padding=1)\n        self.pool1 = torch.nn.MaxPool2d(2)\n        self.conv2 = torch.nn.Conv2d(6,16,5,padding=0)\n        self.pool2 = torch.nn.MaxPool2d(2)\n        self.linear1 = torch.nn.Linear(400,120)\n        self.linear2 = torch.nn.Linear(120,84)\n        self.linear3 = torch.nn.Linear(84,10)\n    def forward(self,x):\n        c1= F.relu(self.conv1(x))\n        s1 = self.pool1(c1)\n        c2 = F.relu(self.conv2(s1))\n        s2 = self.pool2(c2)\n        f  = (Flatten()(s2))\n        f1 = F.relu(self.linear1(f))\n        f2 = F.relu(self.linear2(f1))\n        f3 = self.linear3(f2)\n        return f3","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install Necessary Packages\n!pip install torchsummary","execution_count":5,"outputs":[{"output_type":"stream","text":"Collecting torchsummary\n  Downloading https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Selecting the device and initiating the model**\ntorch summary provides a detailed picture on the arrangement of layers in the resulting neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torchsummary import summary\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = NN().to(device)\nsummary(model,(1,28,28))","execution_count":6,"outputs":[{"output_type":"stream","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 6, 28, 28]              60\n         MaxPool2d-2            [-1, 6, 14, 14]               0\n            Conv2d-3           [-1, 16, 10, 10]           2,416\n         MaxPool2d-4             [-1, 16, 5, 5]               0\n            Linear-5                  [-1, 120]          48,120\n            Linear-6                   [-1, 84]          10,164\n            Linear-7                   [-1, 10]             850\n================================================================\nTotal params: 61,610\nTrainable params: 61,610\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.06\nParams size (MB): 0.24\nEstimated Total Size (MB): 0.30\n----------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Reading the contents of the csv file \nReshaping the input from a single dimensional vector to a image of size (1,28,28)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n\n# Retrieve the features and label columns into a separate numpy arrays\nfeatures = train[train.columns[1:]].values\nlabel = train.label.values\n\n[rows, columns] = features.shape\nprint(type(features))\nprint(type(label))\n\nprint(features.shape)\nprint(label.shape)","execution_count":7,"outputs":[{"output_type":"stream","text":"<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n(42000, 784)\n(42000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = features /255.0\nfeatures = features.reshape(rows,1,28,28)\nprint(features.shape)","execution_count":8,"outputs":[{"output_type":"stream","text":"(42000, 1, 28, 28)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Defining the dataset.** A Class which accepts the input images and the labels; can be used by loaders to feed the images to the network in batches"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nclass mnistDataset(Dataset):\n    def __init__(self, images, labels):\n        self.image =  torch.from_numpy(images)\n        self.gt = torch.from_numpy(labels)\n\n    def __len__(self):\n        #print(self.image.shape)\n        #print(self.gt.shape)\n        return self.image.shape[0]\n\n    def __getitem__(self,index):\n        return self.image[index], self.gt[index]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = mnistDataset(features, label)\nprint(features.shape)\nprint(label.shape)\ntrainLoader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=False)\n","execution_count":10,"outputs":[{"output_type":"stream","text":"(42000, 1, 28, 28)\n(42000,)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Define the Criterion and Optimizer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim\nCriterion = torch.nn.CrossEntropyLoss(reduction='mean')\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train the model **\n1. Obtain the model prediction for batch from the dataset\n2. Compare target against prediction\n3. Obtain gradients and adjust weights based on the loss\n4. Repeat for all the batches in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, Criterion, optimizer, trainLoader):\n    model.train()\n    epoch_list=[]\n    loss_list=[]\n    for epoch in range(100):\n        running_loss = 0\n        for data,target in trainLoader:\n            x = data.to(device)\n            x = x.type(torch.cuda.FloatTensor)\n            y = target.to(device)\n            #Compute model ouput\n            pred = model(x)\n            #print(pred)\n            #print(y)\n            #Compute loss\n            loss = Criterion(pred,y)\n            running_loss +=loss.item()\n            #Optimizer to adjust weights\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        print('epoch:',epoch,'loss:',running_loss/len(trainLoader))\n        epoch_list.append(epoch)\n        loss_list.append(running_loss/len(trainLoader))\n    return epoch_list,loss_list, model\n","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch_list, loss_list, model = train(model, Criterion, optimizer, trainLoader)\n\n","execution_count":31,"outputs":[{"output_type":"stream","text":"epoch: 0 loss: 0.05425375996665719\nepoch: 1 loss: 0.04167670352079269\nepoch: 2 loss: 0.032803924505531136\nepoch: 3 loss: 0.026820856530216704\nepoch: 4 loss: 0.022629876752293398\nepoch: 5 loss: 0.01748613214356746\nepoch: 6 loss: 0.015393423027997769\nepoch: 7 loss: 0.015056835170350454\nepoch: 8 loss: 0.013032091560042449\nepoch: 9 loss: 0.01287550915172361\nepoch: 10 loss: 0.00942571387201023\nepoch: 11 loss: 0.007605080150201227\nepoch: 12 loss: 0.009535423083696554\nepoch: 13 loss: 0.009091621458279204\nepoch: 14 loss: 0.006274964458215124\nepoch: 15 loss: 0.007846063320310312\nepoch: 16 loss: 0.007427121418844698\nepoch: 17 loss: 0.006899247808713254\nepoch: 18 loss: 0.004447393194597235\nepoch: 19 loss: 0.0038884090944897916\nepoch: 20 loss: 0.0031601758521075545\nepoch: 21 loss: 0.0038767283936359405\nepoch: 22 loss: 0.0028971700353043212\nepoch: 23 loss: 0.0012232171103787827\nepoch: 24 loss: 0.00616407317792211\nepoch: 25 loss: 0.002161250412978793\nepoch: 26 loss: 0.0007741667924193575\nepoch: 27 loss: 0.0007703955404684577\nepoch: 28 loss: 0.0012975307226003674\nepoch: 29 loss: 0.0033126300016379216\nepoch: 30 loss: 0.0013323132323240353\nepoch: 31 loss: 0.0018447512330512829\nepoch: 32 loss: 0.008755450115313276\nepoch: 33 loss: 0.0020991120212574188\nepoch: 34 loss: 0.0008717706955537284\nepoch: 35 loss: 0.0005309415738882198\nepoch: 36 loss: 0.00019788813381905714\nepoch: 37 loss: 7.341076062692324e-05\nepoch: 38 loss: 5.1770301042442806e-05\nepoch: 39 loss: 4.246087305649127e-05\nepoch: 40 loss: 3.7277585089471795e-05\nepoch: 41 loss: 3.342147101539879e-05\nepoch: 42 loss: 3.041265120829383e-05\nepoch: 43 loss: 2.7928238775362368e-05\nepoch: 44 loss: 2.5871594839092012e-05\nepoch: 45 loss: 2.4115062914074905e-05\nepoch: 46 loss: 2.263243996106028e-05\nepoch: 47 loss: 2.1306605520377633e-05\nepoch: 48 loss: 2.0145393589611364e-05\nepoch: 49 loss: 1.911456244741993e-05\nepoch: 50 loss: 1.8190361178559063e-05\nepoch: 51 loss: 1.7355101444463475e-05\nepoch: 52 loss: 1.6593138375284865e-05\nepoch: 53 loss: 1.590762820552324e-05\nepoch: 54 loss: 1.5272049677083118e-05\nepoch: 55 loss: 1.4687969591601504e-05\nepoch: 56 loss: 1.4149665808345514e-05\nepoch: 57 loss: 1.3642878721163193e-05\nepoch: 58 loss: 1.3178938912522161e-05\nepoch: 59 loss: 1.2742269630123815e-05\nepoch: 60 loss: 1.233514145849448e-05\nepoch: 61 loss: 1.1954534613791912e-05\nepoch: 62 loss: 1.1604513460272058e-05\nepoch: 63 loss: 1.126700349688732e-05\nepoch: 64 loss: 1.095176874651977e-05\nepoch: 65 loss: 1.0657128867782109e-05\nepoch: 66 loss: 1.0376158225086772e-05\nepoch: 67 loss: 1.0110196602932136e-05\nepoch: 68 loss: 9.858108691836216e-06\nepoch: 69 loss: 9.618940821515107e-06\nepoch: 70 loss: 9.391285130408142e-06\nepoch: 71 loss: 9.174869175472807e-06\nepoch: 72 loss: 8.97130513202848e-06\nepoch: 73 loss: 8.773576637591553e-06\nepoch: 74 loss: 8.589313138233344e-06\nepoch: 75 loss: 8.409295779587399e-06\nepoch: 76 loss: 8.238633496111854e-06\nepoch: 77 loss: 8.07367052897708e-06\nepoch: 78 loss: 7.915837446043655e-06\nepoch: 79 loss: 7.763272229977605e-06\nepoch: 80 loss: 7.6191766083054194e-06\nepoch: 81 loss: 7.477896548962127e-06\nepoch: 82 loss: 7.342179619708337e-06\nepoch: 83 loss: 7.213774186868569e-06\nepoch: 84 loss: 7.088320595981048e-06\nepoch: 85 loss: 6.967158544550026e-06\nepoch: 86 loss: 6.850560507981554e-06\nepoch: 87 loss: 6.737095955884342e-06\nepoch: 88 loss: 6.62774131851187e-06\nepoch: 89 loss: 6.5224057174956006e-06\nepoch: 90 loss: 6.41949972507153e-06\nepoch: 91 loss: 6.3203175877409495e-06\nepoch: 92 loss: 6.223565058261572e-06\nepoch: 93 loss: 6.130104981761655e-06\nepoch: 94 loss: 6.0392561411741025e-06\nepoch: 95 loss: 5.950587161033619e-06\nepoch: 96 loss: 5.864279608169503e-06\nepoch: 97 loss: 5.78099202940729e-06\nepoch: 98 loss: 5.700815279664219e-06\nepoch: 99 loss: 5.6212061726091366e-06\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nclass mnisttestDataset(Dataset):\n    def __init__(self, images):\n        self.image =  torch.from_numpy(images)\n\n    def __len__(self):\n        #print(self.image.shape)\n        #print(self.gt.shape)\n        return self.image.shape[0]\n\n    def __getitem__(self,index):\n        return self.image[index]","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n#Retrieve the features and label columns into a separate numpy arrays\nfeatures = test[test.columns[0:]].values\nprint(features.shape)\nfeatures = features /255.0\nfeatures = features.reshape(28000,1,28,28)\n\ndataset = mnisttestDataset(features)\nprint(features.shape)\nprint(label.shape)\ntestLoader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n","execution_count":37,"outputs":[{"output_type":"stream","text":"(28000, 784)\n(28000, 1, 28, 28)\n(42000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom torch.autograd import Variable\ndef validation(model, optimizer, testLoader, device, Criterion):\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for vinput in testLoader:\n            vinput = Variable(vinput)\n            vinput = vinput.to(device)\n            vinput = vinput.type(torch.cuda.FloatTensor)\n            optimizer.zero_grad()\n            vpredict = model(vinput)\n            predict = F.softmax(vpredict, dim=1)\n            predict = predict.to('cpu')\n            predict = predict.numpy()\n            #print(predict)\n            predictions.append(np.argmax(predict, axis=1))\n            #print(predictions)\n    return predictions","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = validation(model, optimizer, testLoader, device, Criterion)\npredict_labels = []\n\nfor i in range(28000):\n    predict_labels.append(predict[i][0])\n\n\n\nsubmission = pd.read_csv('../input/digit-recognizer/sample_submission.csv')\nsubmission['Label'] = predict_labels\nsubmission.to_csv('submission.csv', index=False)","execution_count":50,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model 4:**\nFinetune Neural Network 'ResNet18 Model'"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision.models as models\n#Instantiating ResNet model with pretrained weights\nmodel = models.resnet18(pretrained = True)\nnum_features = model.fc.in_features\n#Modifying the fully connected layer to reduce number of classes from 1000 to 2\nmodel.fc = nn.Linear(num_features, 2)\nmodel = model.to(device)","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n\n# Retrieve the features and label columns into a separate numpy arrays\nfeatures = train[train.columns[1:]].values\nlabel = train.label.values\n\n[rows, columns] = features.shape\nprint(type(features))\nprint(type(label))\n\nprint(features.shape)\nprint(label.shape)","execution_count":55,"outputs":[{"output_type":"stream","text":"<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n(42000, 784)\n(42000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = features /255.0\nfeatures = features.reshape(rows,1,28,28)\nprint(features.shape)","execution_count":56,"outputs":[{"output_type":"stream","text":"(42000, 1, 28, 28)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nclass mnistDataset(Dataset):\n    def __init__(self, images, labels):\n        self.image =  torch.from_numpy(images)\n        self.gt = torch.from_numpy(labels)\n\n    def __len__(self):\n        #print(self.image.shape)\n        #print(self.gt.shape)\n        return self.image.shape[0]\n\n    def __getitem__(self,index):\n        return self.image[index], self.gt[index]","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = mnistDataset(features, label)\nprint(features.shape)\nprint(label.shape)\ntrainLoader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=False)","execution_count":58,"outputs":[{"output_type":"stream","text":"(42000, 1, 28, 28)\n(42000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_to_update = model.parameters()\noptimizer = torch.optim.SGD(params_to_update, lr=0.001, momentum=0.9)\nCriterion = nn.CrossEntropyLoss(reduction='mean')","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, Criterion, optimizer, trainLoader):\n    model.train()\n    epoch_list=[]\n    loss_list=[]\n    for epoch in range(100):\n        running_loss = 0\n        for data,target in trainLoader:\n            x = data.to(device)\n            x = x.type(torch.cuda.FloatTensor)\n            y = target.to(device)\n            #Compute model ouput\n            pred = model(x)\n            #print(pred)\n            #print(y)\n            #Compute loss\n            loss = Criterion(pred,y)\n            running_loss +=loss.item()\n            #Optimizer to adjust weights\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        print('epoch:',epoch,'loss:',running_loss/len(trainLoader))\n        epoch_list.append(epoch)\n        loss_list.append(running_loss/len(trainLoader))\n    return epoch_list,loss_list, model","execution_count":61,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reference:**\nhttps://www.kaggle.com/tunguz/mnist-logistic-regression-baseline"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}